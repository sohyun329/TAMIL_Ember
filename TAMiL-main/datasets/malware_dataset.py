from datasets.utils.continual_dataset import ContinualDataset, MalwareDataset, store_masked_loaders
import numpy as np
import os

class MyMalwareDataset(ContinualDataset):
    NAME = 'my_malware'
    SETTING = 'class-il'
    N_CLASSES_PER_TASK = 50  # task당 클래스 개수
    N_TASKS = 11

    def __init__(self, args):
        super().__init__(args)
        base_path = '/home/hansohyun329/FreeMOCA/shared_data/'
        self.data_path = os.path.join(base_path, 'X_train.npy')
        self.label_path = os.path.join(base_path, 'Y_train.npy')
        self.test_data_path = os.path.join(base_path, 'X_test.npy')
        self.test_label_path = os.path.join(base_path, 'Y_test.npy')
        self.class_order = np.load(os.path.join(base_path, 'class_order.npy'))

        self.train_full = MalwareDataset(self.data_path, self.label_path)
        self.test_full = MalwareDataset(self.test_data_path, self.test_label_path)

    def get_data_loaders(self):
        return store_masked_loaders(self.train_full, self.test_full, self)

    @staticmethod
    def get_backbone(attention=False, code_dim=256):
        import torch
        import torch.nn as nn

        # AutoEncoder: latent 차원(code_dim) 사용
        class SimpleAutoencoder(nn.Module):
            def __init__(self, feat_dim):
                super().__init__()
                self.encoder = nn.Linear(feat_dim, feat_dim)
                self.decoder = nn.Linear(feat_dim, feat_dim)
            def forward(self, x):
                z = self.encoder(x)
                x_hat = self.decoder(z)
                return x_hat

        class MLP(nn.Module):
            def __init__(self, in_dim=2381, code_dim=256, num_classes=550, n_tasks=11):
                super().__init__()
                self.features_mlp = nn.Sequential(
                    nn.Linear(in_dim, 512),
                    nn.ReLU(),
                    nn.Linear(512, code_dim),
                    nn.ReLU(),
                )
                self.ae = nn.ModuleList([SimpleAutoencoder(code_dim) for _ in range(n_tasks)])
                self.linear = nn.Linear(code_dim, num_classes)

            def features(self, x):
                return self.features_mlp(x)
            def forward(self, x):
                z = self.features(x)
                return self.linear(z)

        # 실험 total 클래스 수에 맞게 num_classes/n_tasks/in_dim 조정!
        return MLP(
            in_dim=2381,         # <--- 데이터 feature 차원 수, 본인 데이터에 맞게
            code_dim=256,        # <--- latent(잠재)차원, 128~512에서 적당히
            num_classes=MyMalwareDataset.N_CLASSES_PER_TASK * MyMalwareDataset.N_TASKS, # 전체 클래스 개수
            n_tasks=MyMalwareDataset.N_TASKS
        )

    @staticmethod
    def get_transform():
        return None

    @staticmethod
    def get_loss():
        import torch.nn.functional as F
        return F.cross_entropy

    @staticmethod
    def get_normalization_transform():
        return None

    @staticmethod
    def get_denormalization_transform():
        return None